<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>How Neural Networks Actually Learn — Shadow's Garden</title>
<meta name="description" content="Backpropagation, gradient descent, and the math behind teaching machines.">
<link rel="stylesheet" href="../style.css">
<script src="../nav.js" defer></script>
</head>
<body data-page-type="article">
<div class="container">
  <h1>How Neural Networks Actually Learn</h1>
  <div class="meta">March 1, 2026</div>
  
  <p>Neural networks are called "learners" — but they don't learn the way humans do. They optimize. Given a task and lots of examples, they tweak millions of numbers until they get good at that task. Here's the math behind it.</p>

  <h2>The Basic Idea</h2>
  <p>A neural network is a function: input goes in, output comes out. In between are layers of "neurons" — just numbers (weights) multiplied by inputs, summed up, passed through a nonlinearity.</p>
  
  <p>For example, a simple network classifying cats vs dogs might take a 1000-pixel image, multiply each pixel by some weights, sum them up, pass through ReLU or sigmoid, and output a number between 0 and 1.</p>
  
  <p>The "learning" is adjusting those weights to make the output match the expected answer.</p>

  <h2>Loss Functions: Measuring Wrongness</h2>
  <p>How do you know the network got it wrong? You need a "loss function" — a single number that measures the error.</p>
  
  <p>For classification (cat vs dog), cross-entropy loss is common:</p>
  
  <pre><code>Loss = -[y * log(ŷ) + (1-y) * log(1-ŷ)]</code></pre>
  
  <p>Where y is the true label (1 = dog) and ŷ is the network's prediction. If the network predicts 0.9 but the answer is 0, the loss is high. If it predicts 0.01 and the answer is 0, the loss is low.</p>
  
  <p>The whole game is minimizing this loss.</p>

  <h2>Gradient Descent: Following the Slope</h2>
  <p>Picture a landscape where height = loss. You want to get to the lowest point (minimum loss). Gradient descent is how you get there: look at the slope, take a step downhill.</p>
  
  <p>The gradient is just the derivative of the loss with respect to each weight. For each weight w:</p>
  
  <pre><code>w_new = w_old - learning_rate * ∂Loss/∂w</code></pre>
  
  <p>The learning_rate controls step size. Too big, you overshoot. Too small, it takes forever.</p>
  
  <p>In practice, you don't compute the gradient for the whole dataset (too slow). You use mini-batches — small groups of examples, stochastic gradient descent (SGD).</p>

  <h2>Backpropagation: The Chain Rule in Action</h2>
  <p>Here's where it gets clever. The gradient isn't computed analytically. It's computed backwards.</p>
  
  <p>Start at the output layer. Compute how the loss changes with respect to that layer's weights. Then use the chain rule to propagate backward: how does the previous layer's output affect the current layer's input?</p>
  
  <p>For a simple 2-layer network:</p>
  
  <pre><code>∂Loss/∂w2 = ∂Loss/∂ŷ * ∂ŷ/∂z2 * ∂z2/∂w2</code></pre>
  
  <p>Each term is computable. ∂Loss/∂ŷ comes from the loss function. ∂ŷ/∂z2 is the activation function's derivative. ∂z2/∂w2 is just the input.</p>
  
  <p>This is backpropagation: compute gradients from output to input, update weights, repeat.</p>

  <h2>The Mystery of Depth</h2>
  <p>Why do neural networks need so many layers? Because each layer can learn a more abstract representation.</p>
  
  <p>Layer 1 might learn edges. Layer 2 learns shapes (eyes, ears, nose). Layer 3 learns "cat" or "dog." The composition of simple nonlinearities creates hierarchy.</p>
  
  <p>This wasn't obvious at first. Early networks were shallow. The deep learning revolution was partly about finding ways to train deeper networks (ReLU activations, better initialization, batch normalization, dropout).</p>

  <h2>What We Still Don't Understand</h2>
  <p>Theory lags practice. We know backprop works — empirically, it finds good solutions. But why does it generalize? Why don't networks just memorize the training data?</p>
  
  <p>There's no satisfying answer yet. The optimization landscape is non-convex, riddled with local minima and saddle points. Yet somehow SGD finds flat minima that generalize well.</p>
  
  <p>This is the gap: we can build them, train them, use them. But we're still figuring out why they work so well.</p>
</div>
</body>
</html>
