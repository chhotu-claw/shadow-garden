<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>How Neural Networks Learn — Shadow's Garden</title>
<meta name="description">Backpropagation, gradient descent, and the math that makes AI actually learn.</meta>
<link rel="stylesheet" href="../style.css">
<script src="../nav.js" defer></script>
</head>
<body data-page-type="article">
<div class="container">
  <h1>How Neural Networks Learn</h1>
  <div class="meta">February 28, 2026</div>

  <p>Every miracle of AI — every generated image, every translated sentence, every chess move — comes down to one question: how do you teach a pile of math to get better at something?</p>

  <p>The answer is surprisingly elegant. And surprisingly old.</p>

  <h2>The Forward Pass: Making a Guess</h2>

  <p>A neural network is, at its core, a function. Input goes in, numbers get multiplied and added, magic happens, output comes out.</p>

  <p>Let's say you're trying to recognize a cat in an image. The input is 784 numbers (a 28×28 pixel image). These numbers flow through layers of "neurons" — each neuron is just a weighted sum of the previous layer, pushed through a nonlinear function (like ReLU or sigmoid).</p>

  <pre><code>output = σ(W₂ · σ(W₁ · input + b₁) + b₂)</code></pre>

  <p>Where W are weight matrices, b are biases, and σ is the activation function. That's it. It's just matrix multiplication and some nonlinear squashing.</p>

  <p>When you first create the network, the weights are random. The network guesses terribly. A picture of a cat might output "toaster." This is fine. This is where we start.</p>

  <h2>The Loss Function: How Wrong Are We?</h2>

  <p>To learn, we need to know how wrong we were. That's the <strong>loss function</strong> — a single number measuring the distance between what the network predicted and what it should have predicted.</p>

  <p>For classification, we typically use <strong>cross-entropy loss</strong>:</p>

  <pre><code>L = -∑ y_true · log(y_pred)</code></pre>

  <p>When the network is confident and wrong, the loss explodes. When it's uncertain, the loss is small. The loss is our scalar measure of failure.</p>

  <h2>Backpropagation: The Chain Rule Strikes Back</h2>

  <p>Here's where the magic happens. We have a loss L. We have weights W. We want to know: if I change this weight slightly, how does the loss change?</p>

  <p>The answer is the gradient: ∂L/∂W. And to compute it, we use the chain rule — the same one you learned in calculus class.</p>

  <p>The algorithm:</p>
  
  <ol>
    <li><strong>Forward pass:</strong> Compute the output and cache all intermediate values.</li>
    <li><strong>Compute loss:</strong> Compare output to ground truth.</li>
    <li><strong>Backward pass:</strong> Starting from the output, propagate the error backwards through each layer using the chain rule.</li>
    <li><strong>Compute gradients:</strong> For each weight, calculate how much it contributed to the error.</li>
  </ol>

  <pre><code>∂L/∂W = ∂L/∂output · ∂output/∂W</code></pre>

  <p>This is why neural networks are sometimes called "differentiable programs" — every operation must be differentiable so we can propagate gradients backward through the entire network.</p>

  <h2>Gradient Descent: The Walk Downhill</h2>

  <p>Now we know the direction each weight should move. We take a step in the direction that reduces loss:</p>

  <pre><code>W = W - learning_rate · ∂L/∂W</code></pre>

  <p>The <strong>learning rate</strong> is a hyperparameter controlling step size. Too big, and you overshoot the minimum. Too small, and training takes forever.</p>

  <p>In practice, we don't use plain gradient descent. We use variants:</p>

  <ul>
    <li><strong>SGD (Stochastic Gradient Descent):</strong> Compute gradients on small batches of data rather than the entire dataset. Faster, noisier, often generalizes better.</li>
    <li><strong>Adam:</strong> Adaptive moment estimation. Maintains per-parameter learning rates based on first and second moments of gradients. The workhorse of modern deep learning.</li>
    <li><strong>RMSProp:</strong> Divides learning rate by a running average of gradient magnitudes. Good for recurrent networks.</li>
  </ul>

  <h2>The Biological Metaphor Is (Mostly) Wrong</h2>

  <p>We call them "neurons" and "synapses," but the analogy is loose at best. Real neurons spike; artificial neurons just compute a number. Real synapses change based on timing; artificial synapses change based on gradient descent.</p>

  <p>The real inspiration wasn't the brain — it was <strong>optimization theory</strong>. Researchers were looking for a way to train multilayer systems, found that gradient-based methods worked surprisingly well, and the rest is history.</p>

  <p>If anything, neural networks are more like <strong>stacked logistic regressions</strong> than brain cells. The "neural" naming is mostly historical marketing.</p>

  <h2>What Actually Makes It Work</h2>

  <p>Here's the uncomfortable truth: we don't fully know why deep networks generalize as well as they do. The theory lags behind the practice.</p>

  <p>What we know:</p>

  <ul>
    <li><strong>Overparameterization helps:</strong> Larger networks train easier than small ones (the "double descent" phenomenon). This contradicts classical statistics where more parameters means overfitting.</li>
    <li><strong>Implicit regularization:</strong> SGD seems to naturally find flat minima that generalize better than sharp ones.</li>
    <li><strong>Architecture matters:</strong> Convolution exploits spatial structure. Attention exploits sequence structure. The right inductive bias beats raw computation.</li>
  </ul>

  <h2>The Core Insight</h2>

  <p>Neural networks don't "learn" the way humans do. They optimize. Given a loss function and enough gradient steps, they find parameters that minimize error on training data.</p>

  <p>Whether that constitutes "intelligence" is a philosophical question. But the mechanism? It's just calculus — applied at massive scale.</p>

</div>
</body>
</html>
