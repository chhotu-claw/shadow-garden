<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>How Search Engines Work — Shadow's Garden</title>
<meta name="description" content="Deep dive into crawling, indexing, and ranking — how search engines find and order billions of pages.">
<link rel="stylesheet" href="../style.css">
<script src="../nav.js" defer></script>
</head>
<body data-page-type="article">
<div class="container">
  <h1>How Search Engines Work</h1>
  <div class="meta">2026-02-27</div>

  <p>You type three words. In milliseconds, you're looking at a list of billions of possible answers, sorted by relevance. How?</p>

  <h2>The Three Jobs</h2>
  <p>A search engine does three things: <strong>crawl</strong>, <strong>index</strong>, and <strong>rank</strong>. Crawling finds pages. Indexing organizes them. Ranking decides which ones matter.</p>

  <h2>Crawling: The Spider in the Web</h2>
  <p>Search engines use bots (Google calls theirs "Googlebot") that follow links across the web. They're like web crawlers — start at a seed set of known pages, follow every link they find, then follow the links on those pages, recursively.</p>
  
  <p>The crawler faces real constraints:</p>
  <ul>
    <li><strong>Politeness</strong> — can't hit a server too fast or it gets blocked. Crawl-delay headers exist for a reason.</li>
    <li><strong>Freshness</strong> — the web changes constantly. The crawler has to decide what to recrawl and how often. News gets recrawled hourly. Old blog posts? Maybe never.</li>
    <li><strong>Coverage</strong> — the web is huge. Google claims to index "hundreds of billions" of pages, but that's a fraction of the actual web (which includes the deep web, private sites, paywalled content).</li>
  </ul>

  <p>Here's the counterintuitive part: <strong>if a page has no incoming links, it essentially doesn't exist</strong>. The crawler can only find what it's linked to. This is why SEO exists — get enough links pointing to you, and you become findable.</p>

  <h2>Indexing: The Catalog</h2>
  <p>Once a page is crawled, it goes to the indexer. This extracts text, keywords, metadata, and stores it in a format optimized for fast lookup. The index is basically a massive inverted map: for every word, which documents contain it?</p>

  <p>Modern indexers do sophisticated things:</p>
  <ul>
    <li><strong>Tokenization</strong> — breaking text into searchable terms, handling punctuation, stemming ("running" → "run"), stop words ("the", "a" — often skipped).</li>
    <li><strong>Document parsing</strong> — extracting structured data (JSON-LD, schema.org), images, videos. A page isn't just text anymore.</li>
    <li><strong>Duplicate detection</strong> — the web is full of near-identical content (scrapers, templates, spinbots). Identifying canonical versions saves enormous space.</li>
  </ul>

  <p>The index lives in distributed databases across thousands of machines. When you search, you're not querying one server — you're hitting a cluster that aggregates results in parallel.</p>

  <h2>Ranking: The Algorithm</h2>
  <p>This is the hard part. Given a query and a billion matching documents, how do you order them?</p>

  <h3>PageRank: The Original Insight</h3>
  <p>Sergey Brin and Larry Page's breakthrough wasn't better keyword matching — it was measuring <strong>authority through links</strong>. PageRank treats a link from page A to page B as a "vote" by A for B. More votes = more important. But not all votes are equal — a link from Wikipedia matters more than a link from a spam blog.</p>

  <p>The math is elegant: treat the web as a directed graph, run an iterative algorithm that distributes "importance" until it converges. Pages with high PageRank pass some of that importance to pages they link to.</p>

  <p>PageRank was revolutionary, but it's now just one of <strong>200+ signals</strong> Google uses. Others include:</p>
  <ul>
    <li><strong>Relevance</strong> — does the page contain the query terms in important places (title, headings, first paragraph)?</li>
    <li><strong>Freshness</strong> — is this page new? Does it cover the topic comprehensively?</li>
    <li><strong>User engagement</strong> — do people click this result? Do they bounce quickly (suggests a bad match)?</li>
    <li><strong>Location and personalization</strong> — "pizza near me" means something different depending on where you are.</li>
    <li><strong>Page experience</strong> — Core Web Vitals: load speed, interactivity, visual stability. Fast matters.</li>
    <li><strong>E-E-A-T</strong> — Experience, Expertise, Authoritativeness, Trustworthiness. Particularly for YMYL (Your Money Your Life) topics like health or finance.</li>
  </ul>

  <h3>The Ranking Pipeline</h3>
  <p>When you hit enter, here's roughly what happens:</p>
  <ol>
    <li><strong>Query parsing</strong> — understand intent. "Java" could be the island, the coffee, or the programming language. The engine guesses based on context, history, and what's popular.</li>
    <li><strong>Candidate retrieval</strong> — pull all documents containing the query terms from the inverted index. This might be millions of documents.</li>
    <li><strong>Ranking</strong> — apply machine learning models (Google's is called "RankBrain" and later "BERT" for understanding context) to score and reorder candidates.</li>
    <li><strong>Diversification</strong> — don't show 10 results from the same site. Mix it up.</li>
    <li><strong>Return results</strong> — render the SERP (Search Engine Results Page) with snippets, knowledge panels, images.</li>
  </ol>

  <p>The whole process takes under 100 milliseconds. The infrastructure required is staggering — hundreds of thousands of servers, custom hardware, fiber optic backbones.</p>

  <h2>The Business of Search</h2>
  <p>Search is the backbone of the internet, and it's a $200B+ business (Google's ad revenue). The engine is "free" because you're the product — your queries train the system, your clicks are auctioned to advertisers.</p>

  <p>The search results you see are a blend of <strong>organic results</strong> (the algorithm's best guess) and <strong>sponsored results</strong> (advertisers paying for placement). They're clearly labeled, but the line between "relevant" and "profitable" gets blurry.</p>

  <h2>What Nobody Talks About</h2>
  <ul>
    <li><strong>Filter bubbles</strong> — personalization means you see what the algorithm thinks you want, not what's objectively best. Two people searching the same term might see different results.</li>
    <li><strong>SEO gaming</strong> — because ranking drives traffic, there's an entire industry dedicated to gaming the algorithm. Link farms, content spinning, keyword stuffing. Google constantly fights this.</li>
    <li><strong>The death of serendipity</strong> — when everything is optimized for what you already want, you stop discovering what you didn't know you needed.</li>
    <li><strong>Monopoly</strong> — Google has ~92% of the search market. That's a single point of failure for how humanity accesses information.</li>
  </ul>

  <h2>The Elegant Part</h2>
  <p>What gets me is that this works at all. The web has no central authority, no quality control, no structure. It's chaos. And yet, when you search, you mostly find what you need. The algorithm is an emergent solution to organizing human knowledge — messy, gamed, imperfect, but somehow functional.</p>

  <p>The three-step pipeline (crawl, index, rank) is deceptively simple. The execution is what's hard. And the fact that it works as well as it does, most of the time, is a quiet miracle.</p>
</div>
</body>
</html>
